{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "### Model Development\n",
    "\n",
    "Developing models means dealing with:  \n",
    "1. Simple and multiple linear regression\n",
    "2. Model evaluation using visualization\n",
    "3. Polynomial regression and pipelines\n",
    "4. R-squared and MSE for in-sample evaluation\n",
    "5. Prediction and decision making  \n",
    "\n",
    "Ultimately, you can answer decisive questions like, \"how can you determine  \n",
    "a fair value for a used car?\"  \n",
    "\n",
    "A model can be thought of as a mathematical equation used to predict a value  \n",
    "given one or more other values. They relate **one or more independent variable  \n",
    "to dependent variables**.  \n",
    "\n",
    "Usually the more **relevant data** you have, the more accurate your model is.  \n",
    "For example:  \n",
    "\n",
    "  - You enter the following to your model:  \n",
    "      - `highway-mpg`  \n",
    "      - `curb-weight`  \n",
    "      - `engine-size`  \n",
    "\n",
    "And you should receive an accurate prediction for `price`.  \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "### Linear and Multiple Linear Regression  \n",
    "\n",
    "**Linear regression** will refer to one independent variable, while  \n",
    "**multiple linear regression** refers to multiple independent variables to  \n",
    "make a prediction.\n",
    "\n",
    "The important thing to understand about linear models is that they assume  \n",
    "*homoscedastity*, that the residuals (errors) should have roughly the same  \n",
    "spread across all of the predictor(s).  \n",
    "\n",
    "Your model should be equally **confident** or equally **uncertain** regardless  \n",
    "of where you are along the range of X.\n",
    "\n",
    "### Simple Linear Regression  \n",
    "\n",
    "In simple linear regression you have the following:  \n",
    "  - The *predictor* (independent) variable - **X**  \n",
    "  - The *target* (dependent) variable - **Y**  \n",
    "    - We would like to come up with a linear relationship expressed as the \n",
    "      following:  \n",
    "      $y = b_0 + b_1 x$\n",
    "  - $b_0$: the **intercept**  \n",
    "  - $b_1$: the **slope**  \n",
    "\n",
    "To determine the slope and intercept requires heavy calculations--that can  \n",
    "luckily be abstracted by Python (love this language). But, it's important  \n",
    "to understand what is happening. For this example, we'll consider  \n",
    "`auto_df[\"mpg\"]` our *predictor* and `auto_df[\"price\"]` our target *variable*.  \n",
    "\n",
    "At this point in our modeling, we'll primarily use `LinearRegression` from  \n",
    "the `linear_model` module in the `sklearn` (scikit-learn) library:  \n",
    "\n",
    "  - We'll start by using it to create a LinearRegression object--our model.  \n",
    "  - Assign independent varible(s) (X) and dependent variable (Y), then using  \n",
    "    `fit()` to determine intercept ($b_0$) and slope ($b_1$):  \n",
    "\n",
    "$$\n",
    "\\text{slope} = \\frac{\\sum (x_i - \\bar{x})(y_i - \\bar{y})}{\\sum (x_i - \\bar{x})^2}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\text{intercept} = \\bar{y} - \\text{slope} \\cdot \\bar{x}\n",
    "$$  \n",
    "\n",
    "  - There is no prediction without fitting your data.  \n",
    "  - Finally, using `predict()` to determine a prediction (returning an array).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats as sts\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures, StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "df_data = Path().cwd().parent.parent/\"Data\"/\"Clean_Data\"/\"clean_auto_df.csv\"\n",
    "auto_df = pd.read_csv(df_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "lm = LinearRegression()\n",
    "\n",
    "# X must always be a 2D object\n",
    "X = auto_df[[\"highway-L/100km\"]]\n",
    "Y = auto_df[\"price\"]\n",
    "\n",
    "lm.fit(X, Y)\n",
    "b_int = lm.intercept_\n",
    "b_slope = lm.coef_[0]\n",
    "\n",
    "# Again, X must always be 2D\n",
    "Yhat = lm.predict([[10]])\n",
    "\n",
    "print(b_int, \"\\n\", b_slope, \"\\n\", Yhat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "**SLR Usecases**  \n",
    "\n",
    "SLR might seam rather simple compared to MLR, but it provides critical insight:  \n",
    "- It answers one question clearly, \"**How does this one variable affect the\n",
    "  outcome?**\"  \n",
    "\n",
    "- You get *one* slope and *one* relationship--easier explaining to an audience  \n",
    "  or stakeholder.  \n",
    "\n",
    "- Acts as a baseline, telling you how well a **single feature** performs.  \n",
    "\n",
    "- Could be ideal for low-data situations, especially if there aren't many  \n",
    "  strong predictors.  \n",
    "\n",
    "Ultimately, **SLR** is great if you're trying to explain something, like  \n",
    "potential predicting power for your target variable (EDA). **MLR** is geared  \n",
    "toward *full modeling*, when your target is affected by multiple interracting  \n",
    "variables.  \n",
    "\n",
    "---  \n",
    "\n",
    "### Multiple Linear Regression  \n",
    "\n",
    "This method is used to explain the relationship between:\n",
    "- One continuous target (Y) variable  \n",
    "- Two or more predictor (X) variables  \n",
    "\n",
    "While the same exact functions and principles from SLR are used in MLR, aside from  \n",
    "taking multiple variables for X, the key distinction is that there will be   \n",
    "multiple coefficients generated when running `fit()`.  \n",
    "\n",
    "Additionally, it is best practice to pass a data frame object with column names  \n",
    "corresponding to the column names in X, with corresponding predictor values  \n",
    "for each.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "lm2 = LinearRegression()\n",
    "\n",
    "X2 = auto_df[[\"horsepower\", \"engine-size\", \"fuel-type-gas\", \"highway-L/100km\"]]\n",
    "Y2 = auto_df[\"price\"]\n",
    "\n",
    "lm2.fit(X2, Y2)\n",
    "b0 = lm2.intercept_\n",
    "b1 = lm2.coef_\n",
    "\n",
    "predictor = pd.DataFrame([{\n",
    "    \"horsepower\": 125,\n",
    "    \"engine-size\": 130,\n",
    "    \"fuel-type-gas\": 1,\n",
    "    \"highway-L/100km\": 10\n",
    "}])\n",
    "\n",
    "Yhat2 = lm2.predict(predictor)\n",
    "\n",
    "print(b0, \"\\n\", b1, \"\\n\", Yhat2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "---  \n",
    "\n",
    "### Model Evaluation Using Visualization  \n",
    "\n",
    "An important distinction to note is that visualization is purely a diagnostic  \n",
    "tool. They do not store our `LinearRegression` object or reflect parameters  \n",
    "unless you explicitly extract and apply them.  \n",
    "\n",
    "### SLR Model Visualization  \n",
    "\n",
    "The methods here should sound familiar! And we'll be using a familiar library,  \n",
    "`seaborn`.  \n",
    "\n",
    "**`sns.regplot()`**  \n",
    "\n",
    "This shows us a scatterplot of actual, numerical data with X represting our  \n",
    "independent variable (predictor) and Y representing our dependent variable  \n",
    "(target):  \n",
    "- Interpretation:  \n",
    "  - For relationship and trend assessment.\n",
    "  - **Tight clustering around the line** → strong linear relationship.  \n",
    "  - **Wide scatter around the line** → weaker correlation.  \n",
    "  - **Upward slope** → positive correlation.\n",
    "  - **Downward slope** → negative correlation.\n",
    "  - **Outliers** → noticeable data points far from the line may distort  \n",
    "    fit.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reggression scatterplot\n",
    "sns.regplot(x=\"horsepower\", y=\"price\", data=auto_df)\n",
    "plt.ylim(0,)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "**`sns.residplot()`**  \n",
    "\n",
    "Like the SLR visualization method of `regplot()`, `residplot()` does not rely  \n",
    "on already predicted data, it calculates then visualizes for you.The X-axis  \n",
    "represents the predictor variable, and the Y-axis represents our residuals  \n",
    "(the actual Y value less the predicted value).  \n",
    "\n",
    "*Patterns imply the model is  missing something*.\n",
    "\n",
    "- Interpretation:\n",
    "  - Validate assumptions like linearity and constant variance.  \n",
    "  - **Ideal plot** → residuals randomly scattered around 0 (flat, horizontal \n",
    "    cloud).\n",
    "  - **Bad Signs**:\n",
    "    - **Curved shape** → bell curve, linear model is inapropriate.\n",
    "    - **Fan shape** → tight values when X is low, spread out as X gets higher  \n",
    "      --heteroscedasticity (non-constant variance)\n",
    "    - **Clustered errors** → model is missing some pattern in the data.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Residual scatterplot\n",
    "sns.residplot(x=\"horsepower\", y=\"price\", data=auto_df)\n",
    "plt.title(\"Horsepower Residual\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "As you will see when running the cells above, horespower and price present a  \n",
    "fan-shaped regression and residual. This does **NOT** mean `horsepower` is bad  \n",
    "for modeling, rather that:  \n",
    "- Horsepower and price aren't best modeled with a basic linear term alone.  \n",
    "- The model error grows as horsepower increases.\n",
    "- This basic linear model may underpredict or overpredict incosistently across  \n",
    "  horsepower levels.\n",
    "\n",
    "We can also see (from regression) that the relationship is positive (upward).  \n",
    "\n",
    "---  \n",
    "\n",
    "### MLR Model Visualization  \n",
    "\n",
    "It would be impossible to visualize a model with multiple predictors using a  \n",
    "2D plot. Instead, we evaluate model performance by comparing the distribution  \n",
    "of:  \n",
    "- Actual values (our target: Y)  \n",
    "- Predicted values (ŷ, generated from `model.predict(X)`)  \n",
    "\n",
    "We use `sns.kdeplot()` over the course-recommended `sns.distplot()` because the  \n",
    "latter has been deprecated. Kdeplot() uses Kernel Density Estimation, a  \n",
    "smoothing technique for estimating the **probability density function** of  \n",
    "a continuous variable:\n",
    "- Interpretation:\n",
    "  - **Ideal plot**: two smooth curves that overlap closely.  \n",
    "  - **Bad signs**:\n",
    "    - **Offset predicted curve** → overpredicting (right shift),  \n",
    "      underpredicting (left shift), this is considered \"bias.\"  \n",
    "    - **Wider predicted curve** → less consistent, higher variance than the  \n",
    "      real data.  \n",
    "    - **Narrower predicted curve** → too conservative, not capturing the true  \n",
    "      spread of the data.\n",
    "    - **Different shape** → model missed key structure data like: important  \n",
    "      features, hidden categories or interactions, subpopulations with  \n",
    "      different behaviors.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "lm3 = LinearRegression()\n",
    "\n",
    "X3 = auto_df[[\"horsepower\", \"engine-size\", \"fuel-type-gas\", \"highway-L/100km\"]]\n",
    "Y3 = auto_df[\"price\"]\n",
    "lm3.fit(X3, Y3)\n",
    "\n",
    "Yhat3 = lm3.predict(X3)\n",
    "\n",
    "# KDE plots\n",
    "sns.kdeplot(Y3, label=\"Actual\", fill=True) # fill=True shades area under curve\n",
    "sns.kdeplot(Yhat3, label=\"Predicted\", fill=True)\n",
    "\n",
    "plt.title(\"Distribution of Actual vs Predicted Values\")\n",
    "plt.xlabel(\"Target Variable\")\n",
    "plt.ylabel(\"Density\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c8d4265",
   "metadata": {},
   "source": [
    "--- \n",
    "\n",
    "### Polynomial Regression and Pipelines  \n",
    "\n",
    "Our opportunity to use polynomial regression comes when we see a curved  \n",
    "pattern in our scatter plot (when comparing independent to target) or  \n",
    "regression plot. As mentioned eariler, our data can still be deterministic.  \n",
    "\n",
    "*Note: polynomial regression is not the solution for heteroscedasticity.*  \n",
    "\n",
    "We go from our traditional linear regression formula to this one:  \n",
    "$$\n",
    "y = \\beta_0 + \\beta_1 x + \\beta_2 x^2 + \\beta_3 x^3 + \\cdots +\n",
    "\\beta_n x^n + \\varepsilon\n",
    "$$  \n",
    "\n",
    "**Univariate Polynomial Regression**\n",
    "\n",
    "If we are performing univariate polynomial regression (single feature + target)  \n",
    "then we can use both `NumPy`'s `polyfit()` and `poly1d()` functions, and  \n",
    "`scikit-learn`'s `preprocessing` module to perform polynomial regression.  \n",
    "\n",
    "The former is simple but limited--and when we want to perform multivariate  \n",
    "polynomial regression, we must use the `preprocessing` module.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38761e1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using numpy.polyfit nad numpy.poly1d()\n",
    "p_fit = np.polyfit(auto_df[\"horsepower\"], auto_df[\"price\"], 2) # 3rd arg = degrees\n",
    "p_model = np.poly1d(p_fit)\n",
    "p_yhat = p_model([100])\n",
    "\n",
    "print(p_yhat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51c04117",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using sklearn.preprocessing.PolynomialFeatures()\n",
    "X4 = auto_df[[\"horsepower\"]]\n",
    "Y4 = auto_df[\"price\"]\n",
    "\n",
    "# We could also stndardize X4 right here before poly transforming\n",
    "\n",
    "poly = PolynomialFeatures(degree=2, include_bias=False)\n",
    "X4_poly = poly.fit_transform(X4)\n",
    "\n",
    "lm4 = LinearRegression()\n",
    "lm4.fit(X4_poly, Y4)\n",
    "\n",
    "Yhat4 = lm4.predict(X4_poly)\n",
    "# print(Yhat4)\n",
    "\n",
    "# I am curious what the residual plot looks like now\n",
    "residuals = Y4 - Yhat4\n",
    "\n",
    "plt.scatter(Yhat4, residuals)\n",
    "plt.axhline(y=0, color='red', linestyle='--')\n",
    "plt.xlabel(\"Predicted price\")\n",
    "plt.ylabel(\"Residuals\")\n",
    "plt.title(\"X4, Y4 Residual Plot\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c759b162",
   "metadata": {},
   "source": [
    "**Multivariate Polynomial Regression**\n",
    "\n",
    "It mirrors univariate with few differences--one of them being we generally  \n",
    "want to standardize data at this scale with   \n",
    "`sklearn.preprocessing.StandardScaler`, and another that we should simplify the  \n",
    "process using `sklearn.pipeline.Pipeline`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "035fb32d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X5 = auto_df[[\"horsepower\", \"engine-size\", \"fuel-type-gas\", \"highway-L/100km\"]]\n",
    "Y5 = auto_df[\"price\"]\n",
    "\n",
    "pipeline_argument_structure = [\n",
    "    (\"scale\", StandardScaler()),\n",
    "    (\"poly\", PolynomialFeatures()),\n",
    "    (\"regression\", LinearRegression())\n",
    "]\n",
    "\n",
    "lm5 = Pipeline(pipeline_argument_structure)\n",
    "lm5.fit(X5, Y5)\n",
    "Yhat5 = lm5.predict(X5)\n",
    "\n",
    "residuals2 = Y5 - Yhat5\n",
    "\n",
    "print(Yhat5)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
