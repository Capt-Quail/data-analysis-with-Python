{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "### Model Development\n",
    "\n",
    "Developing models means dealing with:  \n",
    "1. Simple and multiple linear regression\n",
    "2. Model evaluation using visualization\n",
    "3. Polynomial regression and pipelines\n",
    "4. R-squared and MSE for in-sample evaluation\n",
    "5. Prediction and decision making  \n",
    "\n",
    "Ultimately, you can answer decisive questions like, \"how can you determine  \n",
    "a fair value for a used car?\"  \n",
    "\n",
    "A model can be thought of as a mathematical equation used to predict a value  \n",
    "given one or more other values. They relate **one or more independent variable  \n",
    "to dependent variables**.  \n",
    "\n",
    "Usually the more **relevant data** you have, the more accurate your model is.  \n",
    "For example:  \n",
    "\n",
    "  - You enter the following to your model:  \n",
    "      - `highway-mpg`  \n",
    "      - `curb-weight`  \n",
    "      - `engine-size`  \n",
    "\n",
    "And you should receive an accurate prediction for `price`.  \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "### Linear and Multiple Linear Regression  \n",
    "\n",
    "**Linear regression** will refer to one independent variable, while  \n",
    "**multiple linear regression** refers to multiple independent variables to  \n",
    "make a prediction.\n",
    "\n",
    "The important thing to understand about linear models is that they assume  \n",
    "*homoscedastity*, that the residuals (errors) should have roughly the same  \n",
    "spread across all of the predictor(s).  \n",
    "\n",
    "Your model should be equally **confident** or equally **uncertain** regardless  \n",
    "of where you are along the range of X.\n",
    "\n",
    "### Simple Linear Regression  \n",
    "\n",
    "In simple linear regression you have the following:  \n",
    "  - The *predictor* (independent) variable - **X**  \n",
    "  - The *target* (dependent) variable - **Y**  \n",
    "    - We would like to come up with a linear relationship expressed as the \n",
    "      following:  \n",
    "      $y = b_0 + b_1 x$\n",
    "  - $b_0$: the **intercept**  \n",
    "  - $b_1$: the **slope**  \n",
    "\n",
    "To determine the slope and intercept requires heavy calculations--that can  \n",
    "luckily be abstracted by Python (love this language). But, it's important  \n",
    "to understand what is happening. For this example, we'll consider  \n",
    "`auto_df[\"mpg\"]` our *predictor* and `auto_df[\"price\"]` our target *variable*.  \n",
    "\n",
    "At this point in our modeling, we'll primarily use `LinearRegression` from  \n",
    "the `linear_model` module in the `sklearn` (scikit-learn) library:  \n",
    "\n",
    "  - We'll start by using it to create a LinearRegression object--our model.  \n",
    "  - Assign independent varible(s) (X) and dependent variable (Y), then using  \n",
    "    `fit()` to determine intercept ($b_0$) and slope ($b_1$):  \n",
    "\n",
    "$$\n",
    "\\text{slope} = \\frac{\\sum (x_i - \\bar{x})(y_i - \\bar{y})}{\\sum (x_i - \\bar{x})^2}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\text{intercept} = \\bar{y} - \\text{slope} \\cdot \\bar{x}\n",
    "$$  \n",
    "\n",
    "  - There is no prediction without fitting your data.  \n",
    "  - Finally, using `predict()` to determine a prediction (returning an array).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats as sts\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures, StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import mean_squared_error, root_mean_squared_error\n",
    "\n",
    "df_data = Path().cwd().parent.parent/\"Data\"/\"Clean_Data\"/\"clean_auto_df.csv\"\n",
    "auto_df = pd.read_csv(df_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "lm = LinearRegression()\n",
    "\n",
    "# X must always be a 2D object\n",
    "X = auto_df[[\"highway-L/100km\"]]\n",
    "Y = auto_df[\"price\"]\n",
    "\n",
    "lm.fit(X, Y)\n",
    "b_int = lm.intercept_\n",
    "b_slope = lm.coef_[0]\n",
    "\n",
    "# Again, X must always be 2D\n",
    "Yhat = lm.predict([[10]])\n",
    "\n",
    "print(b_int, \"\\n\", b_slope, \"\\n\", Yhat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "**SLR Usecases**  \n",
    "\n",
    "SLR might seam rather simple compared to MLR, but it provides critical insight:  \n",
    "- It answers one question clearly, \"**How does this one variable affect the\n",
    "  outcome?**\"  \n",
    "\n",
    "- You get *one* slope and *one* relationship--easier explaining to an audience  \n",
    "  or stakeholder.  \n",
    "\n",
    "- Acts as a baseline, telling you how well a **single feature** performs.  \n",
    "\n",
    "- Could be ideal for low-data situations, especially if there aren't many  \n",
    "  strong predictors.  \n",
    "\n",
    "Ultimately, **SLR** is great if you're trying to explain something, like  \n",
    "potential predicting power for your target variable (EDA). **MLR** is geared  \n",
    "toward *full modeling*, when your target is affected by multiple interracting  \n",
    "variables.  \n",
    "\n",
    "---  \n",
    "\n",
    "### Multiple Linear Regression  \n",
    "\n",
    "This method is used to explain the relationship between:\n",
    "- One continuous target (Y) variable  \n",
    "- Two or more predictor (X) variables  \n",
    "\n",
    "While the same exact functions and principles from SLR are used in MLR, aside from  \n",
    "taking multiple variables for X, the key distinction is that there will be   \n",
    "multiple coefficients generated when running `fit()`.  \n",
    "\n",
    "Additionally, it is best practice to pass a data frame object with column names  \n",
    "corresponding to the column names in X, with corresponding predictor values  \n",
    "for each.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "lm2 = LinearRegression()\n",
    "\n",
    "X2 = auto_df[[\"horsepower\", \"engine-size\", \"fuel-type-gas\", \"highway-L/100km\"]]\n",
    "Y2 = auto_df[\"price\"]\n",
    "\n",
    "lm2.fit(X2, Y2)\n",
    "b0 = lm2.intercept_\n",
    "b1 = lm2.coef_\n",
    "\n",
    "predictor = pd.DataFrame([{\n",
    "    \"horsepower\": 125,\n",
    "    \"engine-size\": 130,\n",
    "    \"fuel-type-gas\": 1,\n",
    "    \"highway-L/100km\": 10\n",
    "}])\n",
    "\n",
    "Yhat2 = lm2.predict(predictor)\n",
    "\n",
    "print(b0, \"\\n\", b1, \"\\n\", Yhat2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "---  \n",
    "\n",
    "### Model Evaluation Using Visualization  \n",
    "\n",
    "An important distinction to note is that visualization is purely a diagnostic  \n",
    "tool. They do not store our `LinearRegression` object or reflect parameters  \n",
    "unless you explicitly extract and apply them.  \n",
    "\n",
    "### SLR Model Visualization  \n",
    "\n",
    "The methods here should sound familiar! And we'll be using a familiar library,  \n",
    "`seaborn`.  \n",
    "\n",
    "**`sns.regplot()`**  \n",
    "\n",
    "This shows us a scatterplot of actual, numerical data with X represting our  \n",
    "independent variable (predictor) and Y representing our dependent variable  \n",
    "(target):  \n",
    "- Interpretation:  \n",
    "  - For relationship and trend assessment.\n",
    "  - **Tight clustering around the line** → strong linear relationship.  \n",
    "  - **Wide scatter around the line** → weaker correlation.  \n",
    "  - **Upward slope** → positive correlation.\n",
    "  - **Downward slope** → negative correlation.\n",
    "  - **Outliers** → noticeable data points far from the line may distort  \n",
    "    fit.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reggression scatterplot\n",
    "sns.regplot(x=\"horsepower\", y=\"price\", data=auto_df)\n",
    "plt.ylim(0,)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "**`sns.residplot()`**  \n",
    "\n",
    "Like the SLR visualization method of `regplot()`, `residplot()` does not rely  \n",
    "on already predicted data, it calculates then visualizes for you.The X-axis  \n",
    "represents the predictor variable, and the Y-axis represents our residuals  \n",
    "(the actual Y value less the predicted value).  \n",
    "\n",
    "*Patterns imply the model is  missing something*.\n",
    "\n",
    "- Interpretation:\n",
    "  - Validate assumptions like linearity and constant variance.  \n",
    "  - **Ideal plot** → residuals randomly scattered around 0 (flat, horizontal \n",
    "    cloud).\n",
    "  - **Bad Signs**:\n",
    "    - **Curved shape** → bell curve, linear model is inapropriate.\n",
    "    - **Fan shape** → tight values when X is low, spread out as X gets higher  \n",
    "      --heteroscedasticity (non-constant variance)\n",
    "    - **Clustered errors** → model is missing some pattern in the data.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Residual scatterplot\n",
    "sns.residplot(x=\"horsepower\", y=\"price\", data=auto_df)\n",
    "plt.title(\"Horsepower Residual\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "As you will see when running the cells above, horespower and price present a  \n",
    "fan-shaped regression and residual. This does **NOT** mean `horsepower` is bad  \n",
    "for modeling, rather that:  \n",
    "- Horsepower and price aren't best modeled with a basic linear term alone.  \n",
    "- The model error grows as horsepower increases.\n",
    "- This basic linear model may underpredict or overpredict incosistently across  \n",
    "  horsepower levels.\n",
    "\n",
    "We can also see (from regression) that the relationship is positive (upward).  \n",
    "\n",
    "---  \n",
    "\n",
    "### MLR Model Visualization  \n",
    "\n",
    "It would be impossible to visualize a model with multiple predictors using a  \n",
    "2D plot. Instead, we evaluate model performance by comparing the distribution  \n",
    "of:  \n",
    "- Actual values (our target: Y)  \n",
    "- Predicted values (ŷ, generated from `model.predict(X)`)  \n",
    "\n",
    "We use `sns.kdeplot()` over the course-recommended `sns.distplot()` because the  \n",
    "latter has been deprecated. Kdeplot() uses Kernel Density Estimation, a  \n",
    "smoothing technique for estimating the **probability density function** of  \n",
    "a continuous variable:\n",
    "- Interpretation:\n",
    "  - **Ideal plot**: two smooth curves that overlap closely.  \n",
    "  - **Bad signs**:\n",
    "    - **Offset predicted curve** → overpredicting (right shift),  \n",
    "      underpredicting (left shift), this is considered \"bias.\"  \n",
    "    - **Wider predicted curve** → less consistent, higher variance than the  \n",
    "      real data.  \n",
    "    - **Narrower predicted curve** → too conservative, not capturing the true  \n",
    "      spread of the data.\n",
    "    - **Different shape** → model missed key structure data like: important  \n",
    "      features, hidden categories or interactions, subpopulations with  \n",
    "      different behaviors.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "lm3 = LinearRegression()\n",
    "\n",
    "X3 = auto_df[[\"horsepower\", \"engine-size\", \"fuel-type-gas\", \"highway-L/100km\"]]\n",
    "Y3 = auto_df[\"price\"]\n",
    "lm3.fit(X3, Y3)\n",
    "\n",
    "Yhat3 = lm3.predict(X3)\n",
    "\n",
    "# KDE plots\n",
    "sns.kdeplot(Y3, label=\"Actual\", fill=True) # fill=True shades area under curve\n",
    "sns.kdeplot(Yhat3, label=\"Predicted\", fill=True)\n",
    "\n",
    "plt.title(\"Distribution of Actual vs Predicted Values\")\n",
    "plt.xlabel(\"Target Variable\")\n",
    "plt.ylabel(\"Density\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c8d4265",
   "metadata": {},
   "source": [
    "--- \n",
    "\n",
    "### Polynomial Regression and Pipelines  \n",
    "\n",
    "Our opportunity to use polynomial regression comes when we see a curved  \n",
    "pattern in our scatter plot (when comparing independent to target) or  \n",
    "regression plot. As mentioned eariler, our data can still be deterministic.  \n",
    "\n",
    "*Note: polynomial regression is not the solution for heteroscedasticity.*  \n",
    "\n",
    "We go from our traditional linear regression formula to this one:  \n",
    "$$\n",
    "y = \\beta_0 + \\beta_1 x + \\beta_2 x^2 + \\beta_3 x^3 + \\cdots +\n",
    "\\beta_n x^n + \\varepsilon\n",
    "$$  \n",
    "\n",
    "**Univariate Polynomial Regression**\n",
    "\n",
    "If we are performing univariate polynomial regression (single feature + target)  \n",
    "then we can use both `NumPy`'s `polyfit()` and `poly1d()` functions, and  \n",
    "`scikit-learn`'s `preprocessing` module to perform polynomial regression.  \n",
    "\n",
    "The former is simple but limited--and when we want to perform multivariate  \n",
    "polynomial regression, we must use the `preprocessing` module.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38761e1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using numpy.polyfit nad numpy.poly1d()\n",
    "p_fit = np.polyfit(auto_df[\"horsepower\"], auto_df[\"price\"], 2) # 3rd arg = degrees\n",
    "p_model = np.poly1d(p_fit)\n",
    "p_yhat = p_model([100])\n",
    "\n",
    "print(p_yhat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51c04117",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using sklearn.preprocessing.PolynomialFeatures()\n",
    "X4 = auto_df[[\"horsepower\"]]\n",
    "Y4 = auto_df[\"price\"]\n",
    "\n",
    "# We could also stndardize X4 right here before poly transforming\n",
    "\n",
    "poly = PolynomialFeatures(degree=2, include_bias=False)\n",
    "X4_poly = poly.fit_transform(X4)\n",
    "\n",
    "lm4 = LinearRegression()\n",
    "lm4.fit(X4_poly, Y4)\n",
    "\n",
    "Yhat4 = lm4.predict(X4_poly)\n",
    "# print(Yhat4)\n",
    "\n",
    "# I am curious what the residual plot looks like now\n",
    "residuals = Y4 - Yhat4\n",
    "\n",
    "plt.scatter(Yhat4, residuals)\n",
    "plt.axhline(y=0, color='red', linestyle='--')\n",
    "plt.xlabel(\"Predicted price\")\n",
    "plt.ylabel(\"Residuals\")\n",
    "plt.title(\"X4, Y4 Residual Plot\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c759b162",
   "metadata": {},
   "source": [
    "**Multivariate Polynomial Regression**\n",
    "\n",
    "It mirrors univariate with few differences--one of them being we generally  \n",
    "want to standardize data at this scale with   \n",
    "`sklearn.preprocessing.StandardScaler`, and another that we should simplify the  \n",
    "process using `sklearn.pipeline.Pipeline`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "035fb32d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X5 = auto_df[[\"horsepower\", \"engine-size\", \"fuel-type-gas\", \"highway-L/100km\"]]\n",
    "Y5 = auto_df[\"price\"]\n",
    "\n",
    "pipeline_argument_structure = [\n",
    "    (\"scale\", StandardScaler()),\n",
    "    (\"poly\", PolynomialFeatures(degree=2, include_bias=False)),\n",
    "    (\"regression\", LinearRegression())\n",
    "]\n",
    "\n",
    "lm5 = Pipeline(pipeline_argument_structure)\n",
    "lm5.fit(X5, Y5)\n",
    "Yhat5 = lm5.predict(X5)\n",
    "\n",
    "residuals2 = Y5 - Yhat5\n",
    "\n",
    "# print(Yhat5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "971cd114",
   "metadata": {},
   "source": [
    "---  \n",
    "\n",
    "### **Measures for In-Sample Evaluation**  \n",
    "\n",
    "As you may have guessed, there must be a way to numerically determine how good  \n",
    "the model fits on the data set. Here are two important measures to determine  \n",
    "model fit:  \n",
    "\n",
    "- **Mean Squared Error**  \n",
    "  - We find the difference between the actual y-value and the predicted value  \n",
    "    (yhat) then square it.\n",
    "  - Found with the `mean_Squared_error()` fxn in `scikit-learn.metrics`.  \n",
    "  - Alone it is hard to interpret; you can take the square root of your MSE  \n",
    "    to get the RMSE.\n",
    "- **R-squared (R^2)**\n",
    "  - Also called the **Coefficient of Determination**, it determines how close  \n",
    "    the data is fitted to the regression line.  \n",
    "  - It returns the percentage of variation of the target variable (Y) that is   \n",
    "    explained by the linear model.\n",
    "  - Performed with the `.score()` method, simply passing actual x-values and  \n",
    "    actual y-value.  \n",
    "  - Results closer to 1 is good, results closer to 0 are poor.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4597425f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determining MSE\n",
    "lm5_mse = mean_squared_error(Y5, Yhat5)\n",
    "\n",
    "# Putting back into original units with RMSE\n",
    "lm5_rmse = root_mean_squared_error(Y5, Yhat5)\n",
    "\n",
    "print(\n",
    "    f\"The mean squared error is: {lm5_mse: .2f}\\n\"\n",
    "    f\"The mean squared error is: {lm5_rmse: .2f}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6b3fc1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determining R^2\n",
    "lm5_r2 = lm5.score(X5, Y5)\n",
    "\n",
    "print(f\"The R-squarred of LM5 is: {lm5_r2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60f6558b",
   "metadata": {},
   "source": [
    "---  \n",
    "\n",
    "### **Prediction and Decision Making**  \n",
    "\n",
    "Up to this point we have wrangled data, explored it, and based on those  \n",
    "outcomes, have modeled it. We got to understand the data, of its  \n",
    "relationship to our target, and how to visualize the output of our predictions.  \n",
    "\n",
    "This final stage before model evalutation is more like a sanity checl--it is  \n",
    "sitting down to systematically look at how your model is constructed, what it's  \n",
    "producing, and how it measures against actual data.  \n",
    "\n",
    "Considering our data, we have done:  \n",
    "\n",
    "- **Simple Linear Regression**\n",
    "  - Uses:\n",
    "    - For when you want to explore how *one* variable affects the target.\n",
    "    - You're doing initial feature investigation (vizualizing residuals, checking  \n",
    "      checking for linearity).  \n",
    "    - You need a baseline model to compare against more complex ones (e.g., \"Is  \n",
    "      measuring against this variable more effective than our MLR model?\").  \n",
    "  - Strengths:  \n",
    "    - Easy to interpret.  \n",
    "    - Fast to compute.  \n",
    "    - Useful for demonstration, intuition, and diagnostic plots.  \n",
    "  - Limits:  \n",
    "    - Does not capture interactions.  \n",
    "    - Can't handle multivariate influence.\n",
    "    - Often underfits real-world data.  \n",
    "\n",
    "- **Multiple Linear Regression**\n",
    "  - Uses:\n",
    "    - You want to build a realistic predictive model.  \n",
    "    - The target is influenced by multiple features.  \n",
    "    - You're trying to reduce error or explain variance more accurately.  \n",
    "    - Your data set can support it.  \n",
    "  - Strengths:\n",
    "    - Captures combined effects of multiple variables.  \n",
    "    - Usually lowers residual error.  \n",
    "    - More flexible and powerful for modeling real-world data.  \n",
    "  - Limits:\n",
    "    - Risk of overfitting with too many or irrelevant features.  \n",
    "    - Harder to interpret without regularization or feature selection.  \n",
    "\n",
    "- **Polynomial Regression**  \n",
    "  *An extension of SLR and MLR*, it applies when:  \n",
    "\n",
    "    - You detect a nonlinear relationship (U or S) in your residual plot.  \n",
    "    - You're trying to capture curvature that linear terms miss.  \n",
    "\n",
    "  And no matter which, it is important to visualize or test our data (other than  \n",
    "  the actual data available). \n",
    "\n",
    "Important to consider about MLR and SLR:  \n",
    "- MSE for MLR will be smaller than the MSE for SLR since the errors of the data  \n",
    "  will decrease when more variables are included in the model.  \n",
    "- Poly regression will also have a smaller MSE than regular regression.  \n",
    "  - Lower MSE does not mean better fit.  \n",
    "- Just as MSE decreases with model complexity, R-squarred tends to increase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d1f0662",
   "metadata": {},
   "outputs": [],
   "source": [
    "# An example of how we want to look at our model to observe performance\n",
    "\n",
    "# Regression plot of our poly MLR to visualize fit between independent \n",
    "# variables + target variable (SLR + MLR)\n",
    "plt.scatter(Y5, Yhat5)\n",
    "plt.xlabel(\"Actual Price (Y5)\")\n",
    "plt.ylabel(\"Predicted Price (Yhat5)\")\n",
    "plt.title(\"Actual vs Predicted Prices\")\n",
    "plt.plot([Y5.min(), Y5.max()], [Y5.min(), Y5.max()], color='red')  # 45° line\n",
    "plt.show()\n",
    "\n",
    "# Residual plot to determine if our model is a good linear fit (SLR + MLR)\n",
    "plt.scatter(Yhat5, residuals2)\n",
    "plt.axhline(y=0, color='red', linestyle='--')\n",
    "plt.xlabel(\"Predicted Price (Yhat5)\")\n",
    "plt.ylabel(\"Residuals (Actual - Predicted)\")\n",
    "plt.title(\"Residual Plot\")\n",
    "plt.show()\n",
    "\n",
    "# KDE plot to view visualize the distribution and shape of a variable, our  \n",
    "# target and our pred in this case, to observe under/overfitting and other \n",
    "# trends (SLR + MLR)\n",
    "sns.kdeplot(Y5, label=\"Actual\", fill=True)\n",
    "sns.kdeplot(Yhat5, label=\"Predicted\", fill=True)\n",
    "\n",
    "plt.title(\"LM5 Poly MLR - Distribution of Actual vs Pred Vals\")\n",
    "plt.xlabel(\"Price\")\n",
    "plt.ylabel(\"Density\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Showing MSE, RMSE, and R^2\n",
    "print(\n",
    "    f\"Here lies the MSE for LM5: {lm5_mse: .2f}\\n\"\n",
    "    f\"Here lies the RMSE for LM5: {lm5_rmse: .2f}\\n\"\n",
    "    f\"Here lies the R-squarred for LM5: {lm5_r2: .2f}\\n\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
