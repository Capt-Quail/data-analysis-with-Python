{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "### Model Development\n",
    "\n",
    "Developing models means dealing with:  \n",
    "1. Simple and multiple linear regression\n",
    "2. Model evaluation using visualization\n",
    "3. Polynomial regression and pipelines\n",
    "4. R-squared and MSE for in-sample evaluation\n",
    "5. Prediction and decision making  \n",
    "\n",
    "Ultimately, you can answer decisive questions like, \"how can you determine  \n",
    "a fair value for a used car?\"  \n",
    "\n",
    "A model can be thought of as a mathematical equation used to predict a value  \n",
    "given one or more other values. They relate **one or more independent variable  \n",
    "to dependent variables**.  \n",
    "\n",
    "Usually the more **relevant data** you have, the more accurate your model is.  \n",
    "For example:  \n",
    "\n",
    "  - You enter the following to your model:  \n",
    "      - `highway-mpg`  \n",
    "      - `curb-weight`  \n",
    "      - `engine-size`  \n",
    "\n",
    "And you should receive an accurate prediction for `price`.  \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "### Linear and Multiple Linear Regression  \n",
    "\n",
    "**Linear regression** will refer to one independent variable, while  \n",
    "**multiple linear regression** refers to multiple independent variables to  \n",
    "make a prediction.\n",
    "\n",
    "The important thing to understand about linear models is that they assume  \n",
    "*homoscedastity*, that the residuals (errors) should have roughly the same  \n",
    "spread across all of the predictor(s).  \n",
    "\n",
    "Your model should be equally **confident** or equally **uncertain** regardless  \n",
    "of where you are along the range of X.\n",
    "\n",
    "### Simple Linear Regression  \n",
    "\n",
    "In simple linear regression you have the following:  \n",
    "  - The *predictor* (independent) variable - **X**  \n",
    "  - The *target* (dependent) variable - **Y**  \n",
    "    - We would like to come up with a linear relationship expressed as the \n",
    "      following:  \n",
    "      $y = b_0 + b_1 x$\n",
    "  - $b_0$: the **intercept**  \n",
    "  - $b_1$: the **slope**  \n",
    "\n",
    "To determine the slope and intercept requires heavy calculations--that can  \n",
    "luckily be abstracted by Python (love this language). But, it's important  \n",
    "to understand what is happening. For this example, we'll consider  \n",
    "`auto_df[\"mpg\"]` our *predictor* and `auto_df[\"price\"]` our target *variable*.  \n",
    "\n",
    "At this point in our modeling, we'll primarily use `LinearRegression` from  \n",
    "the `linear_model` module in the `sklearn` (scikit-learn) library:  \n",
    "\n",
    "  - We'll start by using it to create a LinearRegression object--our model.  \n",
    "  - Assign independent varible(s) (X) and dependent variable (Y), then using  \n",
    "    `fit()` to determine intercept ($b_0$) and slope ($b_1$):  \n",
    "\n",
    "$$\n",
    "\\text{slope} = \\frac{\\sum (x_i - \\bar{x})(y_i - \\bar{y})}{\\sum (x_i - \\bar{x})^2}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\text{intercept} = \\bar{y} - \\text{slope} \\cdot \\bar{x}\n",
    "$$  \n",
    "\n",
    "  - There is no prediction without fitting your data.  \n",
    "  - Finally, using `predict()` to determine a prediction (returning an array).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats as sts\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "df_data = Path().cwd().parent.parent/\"Data\"/\"Clean_Data\"/\"clean_auto_df.csv\"\n",
    "auto_df = pd.read_csv(df_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "lm = LinearRegression()\n",
    "\n",
    "# X must always be a 2D object\n",
    "X = auto_df[[\"highway-L/100km\"]]\n",
    "Y = auto_df[\"price\"]\n",
    "\n",
    "lm.fit(X, Y)\n",
    "b_int = lm.intercept_\n",
    "b_slope = lm.coef_[0]\n",
    "\n",
    "# Again, X must always be 2D\n",
    "Yhat = lm.predict([[10]])\n",
    "\n",
    "print(b_int, \"\\n\", b_slope, \"\\n\", Yhat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "**SLR Usecases**  \n",
    "\n",
    "SLR might seam rather simple compared to MLR, but it provides critical insight:  \n",
    "- It answers one question clearly, \"**How does this one variable affect the\n",
    "  outcome?**\"  \n",
    "\n",
    "- You get *one* slope and *one* relationship--easier explaining to an audience  \n",
    "  or stakeholder.  \n",
    "\n",
    "- Acts as a baseline, telling you how well a **single feature** performs.  \n",
    "\n",
    "- Could be ideal for low-data situations, especially if there aren't many  \n",
    "  strong predictors.  \n",
    "\n",
    "Ultimately, **SLR** is great if you're trying to explain something, like  \n",
    "potential predicting power for your target variable (EDA). **MLR** is geared  \n",
    "toward *full modeling*, when your target is affected by multiple interracting  \n",
    "variables.  \n",
    "\n",
    "---  \n",
    "\n",
    "### Multiple Linear Regression  \n",
    "\n",
    "This method is used to explain the relationship between:\n",
    "- One continuous target (Y) variable  \n",
    "- Two or more predictor (X) variables  \n",
    "\n",
    "While the same exact functions and principles are used in MLR, aside from  \n",
    "taking multiple variables for X, the key distinction is that there will be   \n",
    "multiple coefficients generated when running `fit()`.  \n",
    "\n",
    "Additionally, it is best practice to pass a data frame object with column names  \n",
    "corresponding to the column names in X, with corresponding predictor values  \n",
    "for each.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "lm2 = LinearRegression()\n",
    "\n",
    "X2 = auto_df[[\"horsepower\", \"engine-size\", \"fuel-type-gas\", \"highway-L/100km\"]]\n",
    "Y2 = auto_df[\"price\"]\n",
    "\n",
    "lm2.fit(X2, Y2)\n",
    "b0 = lm2.intercept_\n",
    "b1 = lm2.coef_\n",
    "\n",
    "predictor = pd.DataFrame([{\n",
    "    \"horsepower\": 125,\n",
    "    \"engine-size\": 130,\n",
    "    \"fuel-type-gas\": 1,\n",
    "    \"highway-L/100km\": 10\n",
    "}])\n",
    "\n",
    "Yhat2 = lm2.predict(predictor)\n",
    "\n",
    "print(b0, \"\\n\", b1, \"\\n\", Yhat2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "---  \n",
    "\n",
    "### Model Evaluation Using Visualization  \n",
    "\n",
    "An important distinction to note is that visualization is purely a diagnostic  \n",
    "tool. They do not store our `LinearRegression` object or reflect parameters  \n",
    "unless you explicitly extract and apply them.  \n",
    "\n",
    "### SLR Model Visualization  \n",
    "\n",
    "The methods here should sound familiar! And we'll be using a familiar library,  \n",
    "`seaborn`.  \n",
    "\n",
    "**`sns.regplot()`**  \n",
    "\n",
    "This shows us a scatterplot of actual, numerical data with X represting our  \n",
    "independent variable (predictor) and Y representing our dependent variable  \n",
    "(target):  \n",
    "- Interpretation:  \n",
    "  - For relationship and trend assessment.\n",
    "  - **Tight clustering around the line** → strong linear relationship.  \n",
    "  - **Wide scatter around the line** → weaker correlation.  \n",
    "  - **Upward slope** → positive correlation.\n",
    "  - **Downward slope** → negative correlation.\n",
    "  - **Outliers** → noticeable data points far from the line may distort  \n",
    "    fit.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reggression scatterplot\n",
    "sns.regplot(x=\"horsepower\", y=\"price\", data=auto_df)\n",
    "plt.ylim(0,)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "**`sns.residplot()`**  \n",
    "\n",
    "Like the SLR visualization method of `regplot()`, `residplot()` does not rely  \n",
    "on already predicted data, it calculates then visualizes for you.The X-axis  \n",
    "represents the predictor variable, and the Y-axis represents our residuals  \n",
    "(the actual Y value less the predicted value).  \n",
    "\n",
    "*Patterns imply the model is  missing something*.\n",
    "\n",
    "- Interpretation:\n",
    "  - Validate assumptions like linearity and constant variance.  \n",
    "  - **Ideal plot** → residuals randomly scattered around 0 (flat, horizontal \n",
    "    cloud).\n",
    "  - **Bad Signs**:\n",
    "    - **Curved shape** → bell curve, linear model is inapropriate.\n",
    "    - **Fan shape** → tight values when X is low, spread out as X gets higher  \n",
    "      --heteroscedasticity (non-constant variance)\n",
    "    - **Clustered errors** → model is missing some pattern in the data.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Residual scatterplot\n",
    "sns.residplot(x=\"horsepower\", y=\"price\", data=auto_df)\n",
    "plt.title(\"Horsepower Residual\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "As you will see when running the cells above, horespower and price present a  \n",
    "fan-shaped regression and residual. This does **NOT** mean `horsepower` is bad  \n",
    "for modeling, rather that:  \n",
    "- Horsepower and price aren't best modeled with a basic linear term alone.  \n",
    "- The model error grows as horsepower increases.\n",
    "- This basic linear model may underpredict or overpredict incosistently across  \n",
    "  horsepower levels.\n",
    "\n",
    "We can also see (from regression) that the relationship is positive (upward).  \n",
    "\n",
    "---  \n",
    "\n",
    "### MLR Model Visualization  \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
