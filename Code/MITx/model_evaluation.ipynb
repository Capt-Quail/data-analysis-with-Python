{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ca5763af",
   "metadata": {},
   "source": [
    "### **Model Evaluation and Refinement**  \n",
    "\n",
    "In the following sections we'll learn:  \n",
    "- Model evaluation  \n",
    "- Over-fitting, underfitting, and model selection  \n",
    "- Ridge regression  \n",
    "- Grid search  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71959a6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats as sts\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures, StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import mean_squared_error, root_mean_squared_error\n",
    "\n",
    "df_data = Path().cwd().parent.parent/\"Data\"/\"Clean_Data\"/\"clean_auto_df.csv\"\n",
    "auto_df = pd.read_csv(df_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea64b615",
   "metadata": {},
   "source": [
    "---  \n",
    "\n",
    "### **Model Evaluation**  \n",
    "\n",
    "While in-sample evaluation tells us how well our data fits the data used to  \n",
    "train it, but not how well the trained data can be used to predict new data.  \n",
    "\n",
    "Our solution is to separate our data into **in-sample data** or training data  \n",
    "and **out of sample data** or a test set.  \n",
    "\n",
    "- Our test set simulating real-world data.  \n",
    "- Usually a large portion of our data is used for training, lets say 70%,  \n",
    "      and our testing data would be 30%.  \n",
    "  - Build + train model = training data  \n",
    "  - Evaluation (real-world representation) = test data  \n",
    "\n",
    "How do we seperate that data?  \n",
    "\n",
    "- With `train_test_split()` from `scikit-learn.model_selection`!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5a07fb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_data = auto_df.drop(\"price\", axis=1)\n",
    "y_data = auto_df[\"price\"]\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    x_data,\n",
    "    y_data,\n",
    "    test_size=0.3,\n",
    "    random_state=0\n",
    ")\n",
    "\n",
    "split_data = [x_train, x_test, y_train, y_test]\n",
    "data_names = {1: \"x_train\", 2: \"x_test\", 3: \"y_train\", 4: \"y_test\"}\n",
    "loop_count = 0\n",
    "\n",
    "for data_set in split_data:\n",
    "    loop_count += 1\n",
    "    label = data_names[loop_count]\n",
    "    print(f\"The {label} data has a shape of: {data_set.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ee7fa81",
   "metadata": {},
   "source": [
    "- **x_data**: features or independent variables.  \n",
    "- **y_data**: dataset target, auto_df[\"price\"].  \n",
    "- **test_size**: percentage of the data for testing (30% here).  \n",
    "- **random_state**: number generator used for random sampling.  \n",
    "\n",
    "---  \n",
    "\n",
    "### **Generalization Performance**  \n",
    "\n",
    "The goal of using training and then testing our data is a measure of how well  \n",
    "our data does at predicting previously unseen data. \n",
    "\n",
    "- The error we obtain using our testing data is an approximation of this error,  \n",
    "  *genralization performance*.\n",
    "\n",
    "Important to note:  \n",
    "- using a lot of data for training gives us an accurate means of determining  \n",
    "  how our model will perform in the real world, **but the precision will be  \n",
    "  low**\n",
    "- If we use fewer data points to train the model and more to test it, **the  \n",
    "  generalization error will be higher, but the model will have good precision**.  \n",
    "- To overcome this, we use **cross validation**.  \n",
    "  - One of the most common out-of-sample evaluation methods, it splits the  \n",
    "    data set into k-equal groups (called a fold), uses all variations of the  \n",
    "    data to train and test, then produces and array of R^2 scores.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be878891",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Model for cross val\n",
    "lr = LinearRegression()\n",
    "\n",
    "scores = cross_val_score(\n",
    "    lr,\n",
    "    x_data[[\"horsepower\"]],\n",
    "    y_data,\n",
    "    cv=3\n",
    ") # cv = # of folds\n",
    "\n",
    "print(scores)\n",
    "\n",
    "# Mean of R^2\n",
    "np.mean(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73bf5a99",
   "metadata": {},
   "source": [
    "What if we want a little more information, like actual predicted values  \n",
    "supplied by our model *before* the R-squarred values are calculated?  \n",
    "- Enter: `cross_val_predicted()`, which takes the exact same arguments as  \n",
    "  `cross_val_score()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08ff57f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_predict\n",
    "\n",
    "cross_p = cross_val_predict(\n",
    "    lr,\n",
    "    x_data[[\"horsepower\"]],\n",
    "    y_data,\n",
    "    cv=3\n",
    ")\n",
    "\n",
    "cross_p[0:5]"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
