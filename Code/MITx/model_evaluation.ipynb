{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ca5763af",
   "metadata": {},
   "source": [
    "### **Model Evaluation and Refinement**  \n",
    "\n",
    "In the following sections we'll learn:  \n",
    "- Model evaluation  \n",
    "- Over-fitting, underfitting, and model selection  \n",
    "- Ridge regression  \n",
    "- Grid search  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71959a6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats as sts\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures, StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import mean_squared_error, root_mean_squared_error\n",
    "\n",
    "df_data = Path().cwd().parent.parent/\"Data\"/\"Clean_Data\"/\"clean_auto_df.csv\"\n",
    "auto_df = pd.read_csv(df_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea64b615",
   "metadata": {},
   "source": [
    "---  \n",
    "\n",
    "### **Model Evaluation**  \n",
    "\n",
    "While in-sample evaluation tells us how well our data fits the data used to  \n",
    "train it, but not how well the trained data can be used to predict new data.  \n",
    "\n",
    "Our solution is to separate our data into **in-sample data** or training data  \n",
    "and **out of sample data** or a test set.  \n",
    "\n",
    "- Our test set simulating real-world data.  \n",
    "- Usually a large portion of our data is used for training, lets say 70%,  \n",
    "      and our testing data would be 30%.  \n",
    "  - Build + train model = training data  \n",
    "  - Evaluation (real-world representation) = test data  \n",
    "\n",
    "How do we seperate that data?  \n",
    "\n",
    "- With `train_test_split()` from `scikit-learn.model_selection`!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5a07fb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_data = auto_df.drop(\"price\", axis=1)\n",
    "y_data = auto_df[\"price\"]\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    x_data,\n",
    "    y_data,\n",
    "    test_size=0.3,\n",
    "    random_state=0\n",
    ")\n",
    "\n",
    "split_data = [x_train, x_test, y_train, y_test]\n",
    "data_names = {1: \"x_train\", 2: \"x_test\", 3: \"y_train\", 4: \"y_test\"}\n",
    "loop_count = 0\n",
    "\n",
    "for data_set in split_data:\n",
    "    loop_count += 1\n",
    "    label = data_names[loop_count]\n",
    "    print(f\"The {label} data has a shape of: {data_set.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ee7fa81",
   "metadata": {},
   "source": [
    "- **x_data**: features or independent variables.  \n",
    "- **y_data**: dataset target, auto_df[\"price\"].  \n",
    "- **test_size**: percentage of the data for testing (30% here).  \n",
    "- **random_state**: number generator used for random sampling.  \n",
    "\n",
    "---  \n",
    "\n",
    "### **Generalization Performance**  \n",
    "\n",
    "The goal of using training and then testing our data is a measure of how well  \n",
    "our data does at predicting previously unseen data. \n",
    "\n",
    "- The error we obtain using our testing data is an approximation of this error,  \n",
    "  *genralization performance*.\n",
    "\n",
    "Important to note:  \n",
    "- using a lot of data for training gives us an accurate means of determining  \n",
    "  how our model will perform in the real world, **but the precision will be  \n",
    "  low**\n",
    "- If we use fewer data points to train the model and more to test it, **the  \n",
    "  generalization error will be higher, but the model will have good precision**.  \n",
    "- To overcome this, we use **cross validation**.  \n",
    "  - One of the most common out-of-sample evaluation methods, it splits the  \n",
    "    data set into k-equal groups (called a fold), uses all variations of the  \n",
    "    data to train and test, then produces and array of $R^2$ scores.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be878891",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Model for cross val\n",
    "lr = LinearRegression()\n",
    "\n",
    "scores = cross_val_score(\n",
    "    lr,\n",
    "    x_data[[\"horsepower\"]],\n",
    "    y_data,\n",
    "    cv=3\n",
    ") # cv = # of folds\n",
    "\n",
    "print(scores)\n",
    "\n",
    "# Mean of R^2\n",
    "np.mean(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73bf5a99",
   "metadata": {},
   "source": [
    "What if we want a little more information, like actual predicted values  \n",
    "supplied by our model *before* the $R^2$ values are calculated?  \n",
    "- Enter: `cross_val_predicted()`, which takes the exact same arguments as  \n",
    "  `cross_val_score()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08ff57f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_predict\n",
    "\n",
    "cross_p = cross_val_predict(\n",
    "    lr,\n",
    "    x_data[[\"horsepower\"]],\n",
    "    y_data,\n",
    "    cv=3\n",
    ")\n",
    "\n",
    "cross_p[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bc47723",
   "metadata": {},
   "source": [
    "### **Overfitting, Underfitting and Model Selection**  \n",
    "\n",
    "This section will discuss how to pick the best polynomial order and problems  \n",
    "that arise with selecting the wrong order polynomial.  \n",
    "\n",
    "**Underfitting**  \n",
    "*Assuming training points are coming from a polynomial function + some noise,  \n",
    "and our goal of model selection is to determine the order of polynomial*\n",
    "\n",
    "- With a (simple) linear regression model, we see our regression line slash  \n",
    "  through a non linear graph. An obvious sign of underfitting.  \n",
    "- When a model underfits, specifically in the case of applying just linear  \n",
    "  regression, it means the model is too simple to fit the data.  \n",
    "- Underfitting can still happen with lower order polynomial regression, even  \n",
    "  though the model fit may imporve.\n",
    "- We will visually see a better fit when applying higher order polynomial  \n",
    "  regression (assuming chosen features are accurate), especially at inflection  \n",
    "  points.  \n",
    "\n",
    "**Overfitting**  \n",
    "When we move past that \"sweet-spot\" of orders in our polynomial linear  \n",
    "regression model, we start to see overfitting.  \n",
    "\n",
    "- A model overfits when it does extremely well tracking the training points,  \n",
    "  but performs poorly at estimating the correct function (testing data).  \n",
    "- The overfit will be especially dramatic in areas where there is little  \n",
    "  training data; visually, you will see a lot of function oscillation.  \n",
    "- Overall, the function is *too* flexible and fits the noise rather than the  \n",
    "  function.  \n",
    "\n",
    "We can also analyze the $R^2$ from an array of linear polynomial equations.  \n",
    "If we were to plot the training and test error from our equations, we would  \n",
    "most likely observe the following pattern:  \n",
    "- **Test Data**: A decrease in $R^2$ until it reaches its lowest point,  \n",
    "  and increases as x (the order) increases. \n",
    "  - Anything on the left is *underfitting*, anything on the right is  \n",
    "    *overfitting*.\n",
    "- **Training Data**: A linear decline in $R^2$ as the degree increases.  \n",
    "\n",
    "Our test data is what we want to pay attention to, it gives us a better means  \n",
    "of estimating the error of our polynomial. **However**, even when choosing the  \n",
    "best fitting polynomial, we will still have some level of error, or noise.  \n",
    "- Noise is random, we cannot predict all of it. Sometimes, this is referred to  \n",
    "  as **irreducible error**.  \n",
    "    - Other reasons for noise: polynomial assumption might be wrong, or sample  \n",
    "      points may have come from a different function, or for real data, it may  \n",
    "      be too difficult to fit or we may not have the correct type of data.  \n",
    "\n",
    "Below, we'll see how to quickly loop through models with different polynomial  \n",
    "degrees to see which is the best fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ba0dec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "Rsqu_test = []\n",
    "order = [2, 3, 4, 5, 6, 7, 8, 8, 10, 11]\n",
    "\n",
    "for n in order:\n",
    "    pr = PolynomialFeatures(degree=n)\n",
    "\n",
    "    x_train_pr = pr.fit_transform(x_train[[\"horsepower\"]])\n",
    "    x_test_pr = pr.fit_transform(x_test[[\"horsepower\"]])\n",
    "\n",
    "    lr.fit(x_train_pr, y_train)\n",
    "    Rsqu_test.append(lr.score(x_test_pr, y_test))\n",
    "\n",
    "print(Rsqu_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dd3542f",
   "metadata": {},
   "source": [
    "### **Ridge Regression**  \n",
    "\n",
    "Ridge regression prevents overfitting. Whether overfitting comes from many  \n",
    "independent variables or from polynomial regression, ridge regression  \n",
    "minimizes overfitting in our model by managing the **magnitude** (how large the  \n",
    "coefficients are allowed to get) through the **hyperperameter, alpha ($\\alpha$)**.  \n",
    "\n",
    "It works by applying a penalty for those large coefficients, $\\alpha$ being how  \n",
    "large or small that penalty is, to minimize prediction error while keeping the  \n",
    "model simpler and more generalizable.  \n",
    "- `Ridge()` goes beyond \"fitting a line\" by controlling the complexity of that  \n",
    "  that line to avoid chasing noise.\n",
    "\n",
    "As alpha increases, the coefficients for each $x^n$ term shrink toward 0:  \n",
    "- Alpha must be selected carefully.  \n",
    "- **$\\alpha$ = 0**: No penalty, just OLS. Overfitting most evident.  \n",
    "- **Small $\\alpha$**: Light regularization. May still overfit, but more stable.  \n",
    "- **Medium $\\alpha$**: Balanced regularization. Reduces variance, but may  \n",
    "  slightly bias.\n",
    "- **Large $\\alpha$**: Strong regularization, heavily shrinks coefficients. As  \n",
    "  they approach 0, they can under-fit the data.  \n",
    "\n",
    "*Note: Even though the following example will use variables `x_train, y_train, x_test, y_test`,  \n",
    "it is very important to use a completely separate split of the data (**validation data**)  \n",
    "as a subset of `x_train` and `y_train` to calculate `Ridge()`. Only if we are NOT using  \n",
    "`GridSearchCV()`, which calculates cross-val*, eliminating the need to split.  \n",
    "\n",
    "Another important clarification is that if you want to calculate for alpha on  \n",
    "a polynomial function, you must (poly) `fit_transform()` the data first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d04dad5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "from tqdm import tqdm # Progress bar visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "250ec273",
   "metadata": {},
   "source": [
    "I wanted to split the data again just to keep things sanitary since there is  \n",
    "a lot happening with and to our data. Important to note:  \n",
    "- Polynomial equations very quickly scale up, as we know, but it deeply affects  \n",
    "  modeling and visualization.\n",
    "- While scaling and normalization help with scaled values, overfitting is  \n",
    "  likely, which is why Ridge regression is helpful, but:  \n",
    "  - The larger our data scales with polynomial regression and/or dummy  \n",
    "    variables, the more likely our **design matrix** (usually X, contains all  \n",
    "    the input features used in regression) is to become ill-conditioned,  \n",
    "    preventing our model from learning.\n",
    "\n",
    "As it relates to ridge regression, ill-conditioned models break conventional   \n",
    "vizualization patterns, usually signalling:\n",
    "- Exploding polynomial terms from high-degree expansion.\n",
    "- Redundant or near-contsant cross-terms (especially from dummy variables).\n",
    "- Too few rows to support the expanded feature space.\n",
    "- A breakdown in transformation logic (like applying `fit_transform()` to  \n",
    "  **test/validation data**).  \n",
    "\n",
    "For consistency I wanted to use the model I created in a previous example, but  \n",
    "the use of dummy variables and polynomial regression ruin the design matrix,  \n",
    "creating an unreliable model (even with scaling).  \n",
    "\n",
    "Later I will be covering the use of `scitkit-learn.compose`'s `ColumnTransformer`  \n",
    "function and `Pipeline` to overcome these obstacles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b377eea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prep variables for model\n",
    "\n",
    "x_train1, x_test1, y_train1, y_test1 = train_test_split(\n",
    "    x_data, y_data, random_state=0\n",
    ")\n",
    "\n",
    "X1 = x_train1[[\n",
    "    \"horsepower\",\n",
    "    \"engine-size\",\n",
    "    \"fuel-type-gas\",\n",
    "    \"highway-L/100km\"\n",
    "]]\n",
    "\n",
    "X2 = x_test1[[\n",
    "    \"horsepower\",\n",
    "    \"engine-size\",\n",
    "    \"fuel-type-gas\",\n",
    "    \"highway-L/100km\"\n",
    "]]\n",
    "\n",
    "SS = StandardScaler()\n",
    "xtr_scaled = SS.fit_transform(X1)\n",
    "xte_scaled = SS.transform(X2)\n",
    "\n",
    "PR = PolynomialFeatures(degree=2, include_bias=False)\n",
    "xtr_ptf = PR.fit_transform(xtr_scaled)\n",
    "xte_ptf = PR.transform(xte_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6279a43b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modeling and visualization prep\n",
    "\n",
    "Rsqu_test = []\n",
    "Rsqu_train = []\n",
    "dummy1 = []\n",
    "Alpha = 10 * np.array(range(0,1000))\n",
    "pbar = tqdm(Alpha)\n",
    "\n",
    "for alpha in pbar:\n",
    "    RigeModel = Ridge(alpha=alpha) \n",
    "    RigeModel.fit(xtr_ptf, y_train1)\n",
    "    test_score, train_score = (\n",
    "        RigeModel.score(xte_ptf, y_test1),\n",
    "        RigeModel.score(xtr_ptf, y_train1)\n",
    "    )\n",
    "    \n",
    "    pbar.set_postfix({\"Test Score\": test_score, \"Train Score\": train_score})\n",
    "\n",
    "    Rsqu_test.append(test_score)\n",
    "    Rsqu_train.append(train_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c88e7b87",
   "metadata": {},
   "source": [
    "Now we can plot the most effective alpha value!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "732a22cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "width = 12\n",
    "height = 10\n",
    "plt.figure(figsize=(width, height))\n",
    "\n",
    "plt.plot(Alpha,Rsqu_test, label='validation data  ')\n",
    "plt.plot(Alpha,Rsqu_train, 'r', label='training Data ')\n",
    "plt.xlabel('alpha')\n",
    "plt.ylabel('R^2')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca52349c",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
