{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ca5763af",
   "metadata": {},
   "source": [
    "### **Model Evaluation and Refinement**  \n",
    "\n",
    "In the following sections we'll learn:  \n",
    "- Model evaluation  \n",
    "- Over-fitting, underfitting, and model selection  \n",
    "- Ridge regression  \n",
    "- Grid search  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71959a6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats as sts\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures, StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import mean_squared_error, root_mean_squared_error\n",
    "\n",
    "df_data = Path().cwd().parent.parent/\"Data\"/\"Clean_Data\"/\"clean_auto_df.csv\"\n",
    "auto_df = pd.read_csv(df_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea64b615",
   "metadata": {},
   "source": [
    "---  \n",
    "\n",
    "### **Model Evaluation**  \n",
    "\n",
    "While in-sample evaluation tells us how well our data fits the data used to  \n",
    "train it, but not how well the trained data can be used to predict new data.  \n",
    "\n",
    "Our solution is to separate our data into **in-sample data** or training data  \n",
    "and **out of sample data** or a test set.  \n",
    "\n",
    "- Our test set simulating real-world data.  \n",
    "- Usually a large portion of our data is used for training, lets say 70%,  \n",
    "      and our testing data would be 30%.  \n",
    "  - Build + train model = training data  \n",
    "  - Evaluation (real-world representation) = test data  \n",
    "\n",
    "How do we seperate that data?  \n",
    "\n",
    "- With `train_test_split()` from `scikit-learn.model_selection`!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5a07fb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_data = auto_df.drop(\"price\", axis=1)\n",
    "y_data = auto_df[\"price\"]\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    x_data,\n",
    "    y_data,\n",
    "    test_size=0.3,\n",
    "    random_state=0\n",
    ")\n",
    "\n",
    "split_data = [x_train, x_test, y_train, y_test]\n",
    "data_names = {1: \"x_train\", 2: \"x_test\", 3: \"y_train\", 4: \"y_test\"}\n",
    "loop_count = 0\n",
    "\n",
    "for data_set in split_data:\n",
    "    loop_count += 1\n",
    "    label = data_names[loop_count]\n",
    "    print(f\"The {label} data has a shape of: {data_set.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ee7fa81",
   "metadata": {},
   "source": [
    "- **x_data**: features or independent variables.  \n",
    "- **y_data**: dataset target, auto_df[\"price\"].  \n",
    "- **test_size**: percentage of the data for testing (30% here).  \n",
    "- **random_state**: number generator used for random sampling.  \n",
    "\n",
    "---  \n",
    "\n",
    "### **Generalization Performance**  \n",
    "\n",
    "The goal of using training and then testing our data is a measure of how well  \n",
    "our data does at predicting previously unseen data. \n",
    "\n",
    "- The error we obtain using our testing data is an approximation of this error,  \n",
    "  *genralization performance*.\n",
    "\n",
    "Important to note:  \n",
    "- using a lot of data for training gives us an accurate means of determining  \n",
    "  how our model will perform in the real world, **but the precision will be  \n",
    "  low**\n",
    "- If we use fewer data points to train the model and more to test it, **the  \n",
    "  generalization error will be higher, but the model will have good precision**.  \n",
    "- To overcome this, we use **cross validation**.  \n",
    "  - One of the most common out-of-sample evaluation methods, it splits the  \n",
    "    data set into k-equal groups (called a fold), uses all variations of the  \n",
    "    data to train and test, then produces and array of R^2 scores.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be878891",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Model for cross val\n",
    "lr = LinearRegression()\n",
    "\n",
    "scores = cross_val_score(\n",
    "    lr,\n",
    "    x_data[[\"horsepower\"]],\n",
    "    y_data,\n",
    "    cv=3\n",
    ") # cv = # of folds\n",
    "\n",
    "print(scores)\n",
    "\n",
    "# Mean of R^2\n",
    "np.mean(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73bf5a99",
   "metadata": {},
   "source": [
    "What if we want a little more information, like actual predicted values  \n",
    "supplied by our model *before* the R-squarred values are calculated?  \n",
    "- Enter: `cross_val_predicted()`, which takes the exact same arguments as  \n",
    "  `cross_val_score()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08ff57f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_predict\n",
    "\n",
    "cross_p = cross_val_predict(\n",
    "    lr,\n",
    "    x_data[[\"horsepower\"]],\n",
    "    y_data,\n",
    "    cv=3\n",
    ")\n",
    "\n",
    "cross_p[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bc47723",
   "metadata": {},
   "source": [
    "### **Overfitting, Underfitting and Model Selection**  \n",
    "\n",
    "This section will discuss how to pick the best polynomial order and problems  \n",
    "that arise with selecting the wrong order polynomial.  \n",
    "\n",
    "**Underfitting**  \n",
    "*Assuming training points are coming from a polynomial function + some noise,  \n",
    "and our goal of model selection is to determine the order of polynomial*\n",
    "\n",
    "- With a (simple) linear regression model, we see our regression line slash  \n",
    "  through a non linear graph. An obvious sign of underfitting.  \n",
    "- When a model underfits, specifically in the case of applying just linear  \n",
    "  regression, it means the model is too simple to fit the data.  \n",
    "- Underfitting can still happen with lower order polynomial regression, even  \n",
    "  though the model fit may imporve.\n",
    "- We will visually see a better fit when applying higher order polynomial  \n",
    "  regression (assuming chosen features are accurate), especially at inflection  \n",
    "  points.  \n",
    "\n",
    "**Overfitting**  \n",
    "When we move past that \"sweet-spot\" of orders in our polynomial linear  \n",
    "regression model, we start to see overfitting.  \n",
    "\n",
    "- A model overfits when it does extremely well tracking the training points,  \n",
    "  but performs poorly at estimating the correct function (testing data).  \n",
    "- The overfit will be especially dramatic in areas where there is little  \n",
    "  training data; visually, you will see a lot of function oscillation.  \n",
    "- Overall, the function is *too* flexible and fits the noise rather than the  \n",
    "  function.  \n",
    "\n",
    "We can also analyze the R-squarred from an array of linear polynomial equations.  \n",
    "If we were to plot the training and test error from our equations, we would  \n",
    "most likely observe the following pattern:  \n",
    "- **Test Data**: A decrease in R-squarred until it reaches its lowest point,  \n",
    "  and increases as x (the order) increases. \n",
    "  - Anything on the left is *underfitting*, anything on the right is  \n",
    "    *overfitting*.\n",
    "- **Training Data**: A linear decline in R^2 as the degree increases.  \n",
    "\n",
    "Our test data is what we want to pay attention to, it gives us a better means  \n",
    "of estimating the error of our polynomial. **However**, even when choosing the  \n",
    "best fitting polynomial, we will still have some level of error, or noise.  \n",
    "- Noise is random, we cannot predict all of it. Sometimes, this is referred to  \n",
    "  as **irreducible error.  \n",
    "    - Other reasons for noise: polynomial assumption might be wrong, or sample  \n",
    "      points may have come from a different function, or for real data, it may  \n",
    "      be too difficult to fit or we may not have the correct type of data.  \n",
    "\n",
    "Below, we'll see how to quickly loop through models with different polynomial  \n",
    "degrees to see which is the best fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ba0dec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "Rsqu_test = []\n",
    "order = [2, 3, 4, 5, 6, 7, 8, 8, 10, 11]\n",
    "\n",
    "for n in order:\n",
    "    pr = PolynomialFeatures(degree=n)\n",
    "\n",
    "    x_train_pr = pr.fit_transform(x_train[[\"horsepower\"]])\n",
    "    x_test_pr = pr.fit_transform(x_test[[\"horsepower\"]])\n",
    "\n",
    "    lr.fit(x_train_pr, y_train)\n",
    "    Rsqu_test.append(lr.score(x_test_pr, y_test))\n",
    "\n",
    "print(Rsqu_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dd3542f",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
